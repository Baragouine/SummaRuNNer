{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "606fa719",
   "metadata": {},
   "source": [
    "# Train RNN_RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8a5bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "\n",
    "from utils.GloveMgr import GloveMgr\n",
    "from utils.Dataset import Dataset\n",
    "from utils.DataLoader import DataLoader\n",
    "from utils.preprocess_df import preprocess_df\n",
    "\n",
    "#from models.RNN_RNN import RNN_RNN\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824eacf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 150000\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "learning_rate = 1e-3\n",
    "model_name = \"RNN_RNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d09bc3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "GPU 0: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Display the number of available GPUs\n",
    "    print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "    # Display the name of each GPU\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c5786e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\" \n",
    "\n",
    "device = torch.device(dev)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97644745",
   "metadata": {},
   "outputs": [],
   "source": [
    "glovemgr = GloveMgr(\"./data/glove.6B/glove.6B.100d.txt\", vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe745c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(preprocess_df(pd.read_json(\"./data/train.json\"), glovemgr=glovemgr, is_sep_n=True, remove_stop_word=True, stemming=False, trunc=50, padding=50))\n",
    "train_iter = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8b39f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Dataset(preprocess_df(pd.read_json(\"./data/val.json\"), glovemgr=glovemgr, is_sep_n=True, remove_stop_word=True, stemming=False, trunc=50, padding=50))\n",
    "val_iter = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8d4a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.BasicModel import BasicModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN_RNN(BasicModel):\n",
    "    def __init__(self, device, vocab_size, word_embed = None):\n",
    "        super(RNN_RNN, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.word_embedding = nn.Embedding(vocab_size, 100, padding_idx=0)\n",
    "        # Load word embedding if specified\n",
    "        if word_embed is not None:\n",
    "            self.word_embedding = torch.nn.Embedding.from_pretrained(torch.from_numpy(word_embed).float())\n",
    "\n",
    "        # 100 : word2vec embedding size\n",
    "        self.word_GRU = nn.GRU(input_size = 100, hidden_size = 200, batch_first = True, bidirectional = True)\n",
    "        self.sent_GRU = nn.GRU(input_size = 2*200, hidden_size=200, batch_first = True, bidirectional = True)\n",
    "\n",
    "        # 10: relative position range size, with segment size = 10\n",
    "        self.rel_pos_emb = nn.Embedding(10, 100)\n",
    "        self.abs_pos_emb = nn.Embedding(100, 100)\n",
    "\n",
    "        self.Wdoc = nn.Linear(2*200,2*200)\n",
    "        self.bias_doc = nn.Parameter(torch.FloatTensor(2*200).uniform_(-0.1,0.1))\n",
    "\n",
    "        self.Wcontent = nn.Linear(2*200,1,bias=False)\n",
    "        self.Wsalience = nn.Bilinear(2*200,2*200,1,bias=False)\n",
    "        self.Wnovelty = nn.Bilinear(2*200,2*200,1,bias=False)\n",
    "        self.Wabs_pos = nn.Linear(100,1,bias=False)\n",
    "        self.Wrel_pos = nn.Linear(100,1,bias=False)\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(1).uniform_(-0.1,0.1))\n",
    "\n",
    "    def avg_pool1d(self,x,seq_lens):\n",
    "        out = []\n",
    "        for index,t in enumerate(x):\n",
    "            if seq_lens[index] == 0:\n",
    "                t = t[:1]\n",
    "            else:\n",
    "                t = t[:seq_lens[index],:]\n",
    "            t = torch.t(t).unsqueeze(0)\n",
    "            out.append(F.avg_pool1d(t,t.size(2)))\n",
    "        \n",
    "        out = torch.cat(out).squeeze(2)\n",
    "        return out\n",
    "\n",
    "    def forward(self, arr_x):\n",
    "        probs = []\n",
    "\n",
    "        # for each document, compute probabilities\n",
    "        for doc in arr_x:\n",
    "            sent_lens = torch.sum(torch.sign(doc),dim=1).data\n",
    "            x = self.word_embedding(doc)\n",
    "            x = self.word_GRU(x)[0]\n",
    "            x = self.avg_pool1d(x, sent_lens)\n",
    "            x = self.sent_GRU(x)[0]\n",
    "            d = x\n",
    "            d = d.unsqueeze(0)\n",
    "            d = self.avg_pool1d(d, [d[0].shape[0]])\n",
    "            d = torch.tanh(self.Wdoc(d[0]) + self.bias_doc).unsqueeze(0)\n",
    "            prob_doc = []\n",
    "            s = torch.zeros(1,2*200)\n",
    "            s = s.to(self.device)\n",
    "            for position, h in enumerate(x):\n",
    "                h = h.view(1, -1) # resize\n",
    "                # Compute position embedding\n",
    "                abs_pos = Variable(torch.LongTensor([[position]]))\n",
    "                abs_pos = abs_pos.to(self.device)\n",
    "                abs_pos = self.abs_pos_emb(abs_pos).squeeze(0)\n",
    "\n",
    "                # Compute relative position embedding\n",
    "                rel_pos = int(round(position / 10))\n",
    "                rel_pos = Variable(torch.LongTensor([[rel_pos]]))\n",
    "                rel_pos = rel_pos.to(self.device)\n",
    "                rel_pos = self.rel_pos_emb(rel_pos).squeeze(0)\n",
    "\n",
    "                # Compute proba\n",
    "                content = self.Wcontent(h)\n",
    "                salience = self.Wsalience(h, d)\n",
    "                novelty = -1 * self.Wnovelty(h,F.tanh(s))\n",
    "                ap = self.Wabs_pos(abs_pos)\n",
    "                rp = self.Wrel_pos(rel_pos)\n",
    "                prob = torch.sigmoid(content+salience+novelty+ap+rp+self.bias)\n",
    "\n",
    "                prob_doc.append(prob)\n",
    "\n",
    "                s = s + torch.mm(prob,h)\n",
    "\n",
    "            probs.append(torch.tensor(prob_doc, requires_grad=True))\n",
    "\n",
    "        probs = torch.cat(probs)\n",
    "        probs = probs.to(self.device)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "380f8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_RNN(device=device, vocab_size=vocab_size, word_embed=glovemgr.getEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae1a236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN_RNN(\n",
       "  (word_embedding): Embedding(150002, 100)\n",
       "  (word_GRU): GRU(100, 200, batch_first=True, bidirectional=True)\n",
       "  (sent_GRU): GRU(400, 200, batch_first=True, bidirectional=True)\n",
       "  (rel_pos_emb): Embedding(10, 100)\n",
       "  (abs_pos_emb): Embedding(100, 100)\n",
       "  (Wdoc): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (Wcontent): Linear(in_features=400, out_features=1, bias=False)\n",
       "  (Wsalience): Bilinear(in1_features=400, in2_features=400, out_features=1, bias=False)\n",
       "  (Wnovelty): Bilinear(in1_features=400, in2_features=400, out_features=1, bias=False)\n",
       "  (Wabs_pos): Linear(in_features=100, out_features=1, bias=False)\n",
       "  (Wrel_pos): Linear(in_features=100, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bc411ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN_RNN(\n",
       "  (word_embedding): Embedding(150002, 100)\n",
       "  (word_GRU): GRU(100, 200, batch_first=True, bidirectional=True)\n",
       "  (sent_GRU): GRU(400, 200, batch_first=True, bidirectional=True)\n",
       "  (rel_pos_emb): Embedding(10, 100)\n",
       "  (abs_pos_emb): Embedding(100, 100)\n",
       "  (Wdoc): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (Wcontent): Linear(in_features=400, out_features=1, bias=False)\n",
       "  (Wsalience): Bilinear(in1_features=400, in2_features=400, out_features=1, bias=False)\n",
       "  (Wnovelty): Bilinear(in1_features=400, in2_features=400, out_features=1, bias=False)\n",
       "  (Wabs_pos): Linear(in_features=100, out_features=1, bias=False)\n",
       "  (Wrel_pos): Linear(in_features=100, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters(),lr=learning_rate)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c803c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./checkpoints\"):\n",
    "    os.makedirs(\"./checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0991d3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsaid/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([113])  ~ torch.Size([113])\n",
      "batch  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309102/49095268.py:15: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  clip_grad_norm(model.parameters(), 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([137])  ~ torch.Size([137])\n",
      "batch  2\n",
      "torch.Size([66])  ~ torch.Size([66])\n",
      "batch  3\n",
      "torch.Size([126])  ~ torch.Size([126])\n",
      "batch  4\n",
      "torch.Size([78])  ~ torch.Size([78])\n",
      "batch  5\n",
      "torch.Size([103])  ~ torch.Size([103])\n",
      "batch  6\n",
      "torch.Size([115])  ~ torch.Size([115])\n",
      "batch  7\n",
      "torch.Size([103])  ~ torch.Size([103])\n",
      "batch  8\n",
      "torch.Size([85])  ~ torch.Size([85])\n",
      "batch  9\n",
      "torch.Size([112])  ~ torch.Size([112])\n",
      "batch  10\n",
      "torch.Size([64])  ~ torch.Size([64])\n",
      "batch  11\n",
      "torch.Size([89])  ~ torch.Size([89])\n",
      "batch  12\n",
      "torch.Size([91])  ~ torch.Size([91])\n",
      "batch  13\n",
      "torch.Size([92])  ~ torch.Size([92])\n",
      "batch  14\n",
      "torch.Size([114])  ~ torch.Size([114])\n",
      "batch  15\n",
      "torch.Size([118])  ~ torch.Size([118])\n",
      "batch  16\n",
      "torch.Size([135])  ~ torch.Size([135])\n",
      "batch  17\n",
      "torch.Size([71])  ~ torch.Size([71])\n",
      "batch  18\n",
      "torch.Size([86])  ~ torch.Size([86])\n",
      "batch  19\n",
      "torch.Size([152])  ~ torch.Size([152])\n",
      "batch  20\n",
      "torch.Size([124])  ~ torch.Size([124])\n",
      "batch  21\n",
      "torch.Size([126])  ~ torch.Size([126])\n",
      "batch  22\n",
      "torch.Size([62])  ~ torch.Size([62])\n",
      "batch  23\n",
      "torch.Size([98])  ~ torch.Size([98])\n",
      "batch  24\n",
      "torch.Size([127])  ~ torch.Size([127])\n",
      "batch  25\n",
      "torch.Size([86])  ~ torch.Size([86])\n",
      "batch  26\n",
      "torch.Size([83])  ~ torch.Size([83])\n",
      "batch  27\n",
      "torch.Size([179])  ~ torch.Size([179])\n",
      "batch  28\n",
      "torch.Size([75])  ~ torch.Size([75])\n",
      "batch  29\n",
      "torch.Size([98])  ~ torch.Size([98])\n",
      "batch  30\n",
      "torch.Size([95])  ~ torch.Size([95])\n",
      "batch  31\n",
      "torch.Size([107])  ~ torch.Size([107])\n",
      "batch  32\n",
      "torch.Size([134])  ~ torch.Size([134])\n",
      "batch  33\n",
      "torch.Size([102])  ~ torch.Size([102])\n",
      "batch  34\n",
      "torch.Size([71])  ~ torch.Size([71])\n",
      "batch  35\n",
      "torch.Size([88])  ~ torch.Size([88])\n",
      "batch  36\n",
      "torch.Size([94])  ~ torch.Size([94])\n",
      "batch  37\n",
      "torch.Size([121])  ~ torch.Size([121])\n",
      "batch  38\n",
      "torch.Size([102])  ~ torch.Size([102])\n",
      "batch  39\n",
      "torch.Size([89])  ~ torch.Size([89])\n",
      "batch  40\n",
      "torch.Size([77])  ~ torch.Size([77])\n",
      "batch  41\n",
      "torch.Size([91])  ~ torch.Size([91])\n",
      "batch  42\n",
      "torch.Size([79])  ~ torch.Size([79])\n",
      "batch  43\n",
      "torch.Size([85])  ~ torch.Size([85])\n",
      "batch  44\n",
      "torch.Size([85])  ~ torch.Size([85])\n",
      "batch  45\n",
      "torch.Size([62])  ~ torch.Size([62])\n",
      "batch  46\n",
      "torch.Size([89])  ~ torch.Size([89])\n",
      "batch  47\n",
      "torch.Size([93])  ~ torch.Size([93])\n",
      "batch  48\n",
      "torch.Size([139])  ~ torch.Size([139])\n",
      "batch  49\n",
      "torch.Size([106])  ~ torch.Size([106])\n",
      "batch  50\n",
      "torch.Size([86])  ~ torch.Size([86])\n",
      "batch  51\n",
      "torch.Size([117])  ~ torch.Size([117])\n",
      "batch  52\n",
      "torch.Size([169])  ~ torch.Size([169])\n",
      "batch  53\n",
      "torch.Size([75])  ~ torch.Size([75])\n",
      "batch  54\n",
      "torch.Size([76])  ~ torch.Size([76])\n",
      "batch  55\n",
      "torch.Size([94])  ~ torch.Size([94])\n",
      "batch  56\n",
      "torch.Size([109])  ~ torch.Size([109])\n",
      "batch  57\n",
      "torch.Size([90])  ~ torch.Size([90])\n",
      "batch  58\n",
      "torch.Size([125])  ~ torch.Size([125])\n",
      "batch  59\n",
      "torch.Size([130])  ~ torch.Size([130])\n",
      "batch  60\n",
      "torch.Size([87])  ~ torch.Size([87])\n",
      "batch  61\n",
      "torch.Size([117])  ~ torch.Size([117])\n",
      "batch  62\n",
      "torch.Size([97])  ~ torch.Size([97])\n",
      "batch  63\n",
      "torch.Size([103])  ~ torch.Size([103])\n",
      "batch  64\n",
      "torch.Size([76])  ~ torch.Size([76])\n",
      "batch  65\n",
      "torch.Size([112])  ~ torch.Size([112])\n",
      "batch  66\n",
      "torch.Size([100])  ~ torch.Size([100])\n",
      "batch  67\n",
      "torch.Size([87])  ~ torch.Size([87])\n",
      "batch  68\n",
      "torch.Size([67])  ~ torch.Size([67])\n",
      "batch  69\n",
      "torch.Size([105])  ~ torch.Size([105])\n",
      "batch  70\n",
      "torch.Size([82])  ~ torch.Size([82])\n",
      "batch  71\n",
      "torch.Size([128])  ~ torch.Size([128])\n",
      "batch  72\n",
      "torch.Size([141])  ~ torch.Size([141])\n",
      "batch  73\n",
      "torch.Size([105])  ~ torch.Size([105])\n",
      "batch  74\n",
      "torch.Size([122])  ~ torch.Size([122])\n",
      "batch  75\n",
      "torch.Size([129])  ~ torch.Size([129])\n",
      "batch  76\n",
      "torch.Size([109])  ~ torch.Size([109])\n",
      "batch  77\n",
      "torch.Size([124])  ~ torch.Size([124])\n",
      "batch  78\n",
      "torch.Size([122])  ~ torch.Size([122])\n",
      "batch  79\n",
      "torch.Size([176])  ~ torch.Size([176])\n",
      "batch  80\n",
      "torch.Size([105])  ~ torch.Size([105])\n",
      "batch  81\n",
      "torch.Size([102])  ~ torch.Size([102])\n",
      "batch  82\n",
      "torch.Size([131])  ~ torch.Size([131])\n",
      "batch  83\n",
      "torch.Size([115])  ~ torch.Size([115])\n",
      "batch  84\n",
      "torch.Size([135])  ~ torch.Size([135])\n",
      "batch  85\n",
      "torch.Size([90])  ~ torch.Size([90])\n",
      "batch  86\n",
      "torch.Size([88])  ~ torch.Size([88])\n",
      "batch  87\n",
      "torch.Size([120])  ~ torch.Size([120])\n",
      "batch  88\n",
      "torch.Size([97])  ~ torch.Size([97])\n",
      "batch  89\n",
      "torch.Size([70])  ~ torch.Size([70])\n",
      "batch  90\n",
      "torch.Size([85])  ~ torch.Size([85])\n",
      "batch  91\n",
      "torch.Size([105])  ~ torch.Size([105])\n",
      "batch  92\n",
      "torch.Size([193])  ~ torch.Size([193])\n",
      "batch  93\n",
      "torch.Size([190])  ~ torch.Size([190])\n",
      "batch  94\n",
      "torch.Size([104])  ~ torch.Size([104])\n",
      "batch  95\n",
      "torch.Size([120])  ~ torch.Size([120])\n",
      "batch  96\n",
      "torch.Size([87])  ~ torch.Size([87])\n",
      "batch  97\n",
      "torch.Size([103])  ~ torch.Size([103])\n",
      "batch  98\n",
      "torch.Size([162])  ~ torch.Size([162])\n",
      "batch  99\n",
      "torch.Size([89])  ~ torch.Size([89])\n",
      "batch  100\n",
      "torch.Size([163])  ~ torch.Size([163])\n",
      "batch  101\n",
      "torch.Size([108])  ~ torch.Size([108])\n",
      "batch  102\n",
      "torch.Size([103])  ~ torch.Size([103])\n",
      "batch  103\n",
      "torch.Size([139])  ~ torch.Size([139])\n",
      "batch  104\n",
      "torch.Size([63])  ~ torch.Size([63])\n",
      "batch  105\n",
      "torch.Size([100])  ~ torch.Size([100])\n",
      "batch  106\n",
      "torch.Size([133])  ~ torch.Size([133])\n",
      "batch  107\n",
      "torch.Size([107])  ~ torch.Size([107])\n",
      "batch  108\n",
      "torch.Size([154])  ~ torch.Size([154])\n",
      "batch  109\n",
      "torch.Size([120])  ~ torch.Size([120])\n",
      "batch  110\n",
      "torch.Size([107])  ~ torch.Size([107])\n",
      "batch  111\n",
      "torch.Size([76])  ~ torch.Size([76])\n",
      "batch  112\n",
      "torch.Size([63])  ~ torch.Size([63])\n",
      "batch  113\n",
      "torch.Size([84])  ~ torch.Size([84])\n",
      "batch  114\n",
      "torch.Size([110])  ~ torch.Size([110])\n",
      "batch  115\n",
      "torch.Size([62])  ~ torch.Size([62])\n",
      "batch  116\n",
      "torch.Size([82])  ~ torch.Size([82])\n",
      "batch  117\n",
      "torch.Size([71])  ~ torch.Size([71])\n",
      "batch  118\n",
      "torch.Size([92])  ~ torch.Size([92])\n",
      "batch  119\n",
      "torch.Size([111])  ~ torch.Size([111])\n",
      "batch  120\n",
      "torch.Size([119])  ~ torch.Size([119])\n",
      "batch  121\n",
      "torch.Size([106])  ~ torch.Size([106])\n",
      "batch  122\n",
      "torch.Size([99])  ~ torch.Size([99])\n",
      "batch  123\n",
      "torch.Size([105])  ~ torch.Size([105])\n",
      "batch  124\n",
      "torch.Size([101])  ~ torch.Size([101])\n",
      "batch  125\n",
      "torch.Size([175])  ~ torch.Size([175])\n",
      "batch  126\n",
      "torch.Size([78])  ~ torch.Size([78])\n",
      "batch  127\n",
      "torch.Size([91])  ~ torch.Size([91])\n",
      "batch  128\n",
      "torch.Size([83])  ~ torch.Size([83])\n",
      "batch  129\n",
      "torch.Size([102])  ~ torch.Size([102])\n",
      "batch  130\n",
      "torch.Size([86])  ~ torch.Size([86])\n",
      "batch  131\n",
      "torch.Size([77])  ~ torch.Size([77])\n",
      "batch  132\n",
      "torch.Size([63])  ~ torch.Size([63])\n",
      "batch  133\n",
      "torch.Size([120])  ~ torch.Size([120])\n",
      "batch  134\n",
      "torch.Size([69])  ~ torch.Size([69])\n",
      "batch  135\n",
      "torch.Size([105])  ~ torch.Size([105])\n",
      "batch  136\n",
      "torch.Size([94])  ~ torch.Size([94])\n",
      "batch  137\n",
      "torch.Size([94])  ~ torch.Size([94])\n",
      "batch  138\n",
      "torch.Size([80])  ~ torch.Size([80])\n",
      "batch  139\n",
      "torch.Size([96])  ~ torch.Size([96])\n",
      "batch  140\n",
      "torch.Size([106])  ~ torch.Size([106])\n",
      "batch  141\n",
      "torch.Size([68])  ~ torch.Size([68])\n",
      "batch  142\n",
      "torch.Size([100])  ~ torch.Size([100])\n",
      "batch  143\n",
      "torch.Size([125])  ~ torch.Size([125])\n",
      "batch  144\n",
      "torch.Size([104])  ~ torch.Size([104])\n",
      "batch  145\n",
      "torch.Size([98])  ~ torch.Size([98])\n",
      "batch  146\n",
      "torch.Size([113])  ~ torch.Size([113])\n",
      "batch  147\n",
      "torch.Size([103])  ~ torch.Size([103])\n",
      "batch  148\n",
      "torch.Size([94])  ~ torch.Size([94])\n",
      "batch  149\n",
      "torch.Size([87])  ~ torch.Size([87])\n",
      "batch  150\n",
      "torch.Size([70])  ~ torch.Size([70])\n",
      "batch  151\n",
      "torch.Size([145])  ~ torch.Size([145])\n",
      "batch  152\n",
      "torch.Size([81])  ~ torch.Size([81])\n",
      "batch  153\n",
      "torch.Size([92])  ~ torch.Size([92])\n",
      "batch  154\n",
      "torch.Size([104])  ~ torch.Size([104])\n",
      "batch  155\n",
      "torch.Size([90])  ~ torch.Size([90])\n",
      "batch  156\n",
      "torch.Size([168])  ~ torch.Size([168])\n",
      "batch  157\n",
      "torch.Size([69])  ~ torch.Size([69])\n",
      "batch  158\n",
      "torch.Size([112])  ~ torch.Size([112])\n",
      "batch  159\n",
      "torch.Size([126])  ~ torch.Size([126])\n",
      "batch  160\n",
      "torch.Size([113])  ~ torch.Size([113])\n",
      "batch  161\n",
      "torch.Size([102])  ~ torch.Size([102])\n",
      "batch  162\n",
      "torch.Size([54])  ~ torch.Size([54])\n",
      "batch  163\n",
      "torch.Size([70])  ~ torch.Size([70])\n",
      "batch  164\n",
      "torch.Size([117])  ~ torch.Size([117])\n",
      "batch  165\n",
      "torch.Size([98])  ~ torch.Size([98])\n",
      "batch  166\n",
      "torch.Size([56])  ~ torch.Size([56])\n",
      "batch  167\n",
      "torch.Size([71])  ~ torch.Size([71])\n",
      "batch  168\n",
      "torch.Size([85])  ~ torch.Size([85])\n",
      "batch  169\n",
      "torch.Size([85])  ~ torch.Size([85])\n",
      "batch  170\n",
      "torch.Size([83])  ~ torch.Size([83])\n",
      "batch  171\n",
      "torch.Size([114])  ~ torch.Size([114])\n",
      "batch  172\n",
      "torch.Size([79])  ~ torch.Size([79])\n",
      "batch  173\n",
      "torch.Size([145])  ~ torch.Size([145])\n",
      "batch  174\n",
      "torch.Size([119])  ~ torch.Size([119])\n",
      "batch  175\n",
      "torch.Size([104])  ~ torch.Size([104])\n",
      "batch  176\n",
      "torch.Size([136])  ~ torch.Size([136])\n",
      "batch  177\n",
      "torch.Size([97])  ~ torch.Size([97])\n",
      "batch  178\n",
      "torch.Size([60])  ~ torch.Size([60])\n",
      "batch  179\n",
      "torch.Size([164])  ~ torch.Size([164])\n",
      "batch  180\n",
      "torch.Size([92])  ~ torch.Size([92])\n",
      "batch  181\n",
      "torch.Size([69])  ~ torch.Size([69])\n",
      "batch  182\n",
      "torch.Size([133])  ~ torch.Size([133])\n",
      "batch  183\n",
      "torch.Size([124])  ~ torch.Size([124])\n",
      "batch  184\n",
      "torch.Size([122])  ~ torch.Size([122])\n",
      "batch  185\n",
      "torch.Size([137])  ~ torch.Size([137])\n",
      "batch  186\n",
      "torch.Size([93])  ~ torch.Size([93])\n",
      "batch  187\n",
      "torch.Size([132])  ~ torch.Size([132])\n",
      "batch  188\n",
      "torch.Size([95])  ~ torch.Size([95])\n",
      "batch  189\n",
      "torch.Size([103])  ~ torch.Size([103])\n",
      "batch  190\n",
      "torch.Size([86])  ~ torch.Size([86])\n",
      "batch  191\n",
      "torch.Size([93])  ~ torch.Size([93])\n",
      "batch  192\n",
      "torch.Size([111])  ~ torch.Size([111])\n",
      "batch  193\n",
      "torch.Size([94])  ~ torch.Size([94])\n",
      "batch  194\n",
      "torch.Size([81])  ~ torch.Size([81])\n",
      "batch  195\n",
      "torch.Size([98])  ~ torch.Size([98])\n",
      "batch  196\n",
      "torch.Size([76])  ~ torch.Size([76])\n",
      "batch  197\n",
      "torch.Size([97])  ~ torch.Size([97])\n",
      "batch  198\n",
      "torch.Size([88])  ~ torch.Size([88])\n",
      "batch  199\n",
      "torch.Size([122])  ~ torch.Size([122])\n",
      "batch  200\n",
      "torch.Size([107])  ~ torch.Size([107])\n",
      "batch  201\n",
      "torch.Size([97])  ~ torch.Size([97])\n",
      "batch  202\n",
      "torch.Size([88])  ~ torch.Size([88])\n",
      "batch  203\n",
      "torch.Size([112])  ~ torch.Size([112])\n",
      "batch  204\n",
      "torch.Size([126])  ~ torch.Size([126])\n",
      "batch  205\n",
      "torch.Size([107])  ~ torch.Size([107])\n",
      "batch  206\n",
      "torch.Size([65])  ~ torch.Size([65])\n",
      "batch  207\n",
      "torch.Size([87])  ~ torch.Size([87])\n",
      "batch  208\n",
      "torch.Size([67])  ~ torch.Size([67])\n",
      "batch  209\n",
      "torch.Size([106])  ~ torch.Size([106])\n",
      "batch  210\n",
      "torch.Size([122])  ~ torch.Size([122])\n",
      "batch  211\n",
      "torch.Size([116])  ~ torch.Size([116])\n",
      "batch  212\n",
      "torch.Size([50])  ~ torch.Size([50])\n",
      "batch  213\n",
      "torch.Size([119])  ~ torch.Size([119])\n",
      "batch  214\n",
      "torch.Size([93])  ~ torch.Size([93])\n",
      "batch  215\n",
      "torch.Size([145])  ~ torch.Size([145])\n",
      "batch  216\n",
      "torch.Size([157])  ~ torch.Size([157])\n",
      "batch  217\n",
      "torch.Size([114])  ~ torch.Size([114])\n",
      "batch  218\n",
      "torch.Size([116])  ~ torch.Size([116])\n",
      "batch  219\n",
      "torch.Size([87])  ~ torch.Size([87])\n",
      "batch  220\n",
      "torch.Size([145])  ~ torch.Size([145])\n",
      "batch  221\n",
      "torch.Size([103])  ~ torch.Size([103])\n",
      "batch  222\n",
      "torch.Size([74])  ~ torch.Size([74])\n",
      "batch  223\n",
      "torch.Size([87])  ~ torch.Size([87])\n",
      "batch  224\n",
      "torch.Size([62])  ~ torch.Size([62])\n",
      "batch  225\n",
      "torch.Size([53])  ~ torch.Size([53])\n",
      "batch  226\n",
      "torch.Size([102])  ~ torch.Size([102])\n",
      "batch  227\n",
      "torch.Size([101])  ~ torch.Size([101])\n",
      "batch  228\n",
      "torch.Size([122])  ~ torch.Size([122])\n",
      "batch  229\n",
      "torch.Size([73])  ~ torch.Size([73])\n",
      "batch  230\n",
      "torch.Size([66])  ~ torch.Size([66])\n",
      "batch  231\n",
      "torch.Size([60])  ~ torch.Size([60])\n",
      "batch  232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1088: indexSelectSmallIndex: block: [0,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_309102/49095268.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" ~\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_309102/1509452362.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, arr_x)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;31m# Compute proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWcontent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0msalience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWsalience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mnovelty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWnovelty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "t1 = time() \n",
    "for epoch in range(1, epochs+1):\n",
    "    for i,batch in enumerate(train_iter):\n",
    "        print(\"batch \", i)\n",
    "        features = [Variable(torch.LongTensor(batch[i][\"doc\"])).to(device) for i in range(batch_size)]\n",
    "        targets = [Variable(torch.FloatTensor(batch[i][\"labels\"])) for i in range(batch_size)]\n",
    "        targets = torch.cat(targets)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        probs = model(features)\n",
    "        print(probs.shape, \" ~\", targets.shape)\n",
    "        loss = loss_fn(probs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "t2 = time()\n",
    "print(\"Training duration =\", t2-t1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edbae7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d7f428a150b92572ac46240b6d7ae68586908362b054f21341550673eeb77dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
